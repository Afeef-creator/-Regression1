{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3d_bLDNB5tn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 1. What is Simple Linear Regression\n",
        "\t\t- Simple Linear Regression is a statistical method used to find the relationship\n",
        "\t\t- between one independent variable (X) and one dependent variable (Y).\n",
        "\t\t- It assumes a straight-line relationship between X and Y.\n",
        "\n",
        "- 2. What are the key assumptions of Simple Linear Regression\n",
        "\t\t- There is a linear relationship between X and Y\n",
        "\t\t- Errors are independent\n",
        "\t\t- Errors have constant variance (homoscedasticity)\n",
        "\t\t- Errors are normally distributed\n",
        "\n",
        "- 3. What does the coefficient m represent in the equation Y = mX + c\n",
        "\t\t- The coefficient m represents the slope of the line.\n",
        "\t\t- It shows how much Y changes when X increases by one unit.\n",
        "\n",
        "- 4. What does the intercept c represent in the equation Y = mX + c\n",
        "\t\t- The intercept c represents the value of Y when X is equal to zero.\n",
        "\n",
        "- 5. How do we calculate the slope m in Simple Linear Regression\n",
        "\t\t- The slope m is calculated using the least squares method.\n",
        "\t\t- m = Cov(X, Y) / Var(X)\n",
        "\n",
        "- 6. What is the purpose of the least squares method in Simple Linear Regression\n",
        "\t\t- The least squares method minimizes the sum of squared errors.\n",
        "\t\t- It reduces the difference between actual and predicted values.\n",
        "\n",
        "- 7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression\n",
        "\t\t- R² shows how much variation in the dependent variable\n",
        "\t\t- is explained by the independent variable.\n",
        "\n",
        "- 8. What is Multiple Linear Regression\n",
        "\t\t- Multiple Linear Regression explains the relationship\n",
        "\t\t- between one dependent variable and multiple independent variables.\n",
        "\n",
        "- 9. What is the main difference between Simple and Multiple Linear Regression\n",
        "\t\t- Simple Linear Regression uses one independent variable.\n",
        "\t\t- Multiple Linear Regression uses more than one variable.\n",
        "\n",
        "- 10. What are the key assumptions of Multiple Linear Regression\n",
        "\t\t- Linear relationship between variables\n",
        "\t\t- No multicollinearity\n",
        "\t\t- Homoscedasticity\n",
        "\t\t- Normal distribution of errors\n",
        "\t\t- Independence of errors\n",
        "\n",
        "- 11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n",
        "\t\t- Heteroscedasticity means unequal variance of errors.\n",
        "\t\t- It makes regression results unreliable.\n",
        "\n",
        "- 12. How can you improve a Multiple Linear Regression model with high multicollinearity\n",
        "\t\t- Remove highly correlated variables.\n",
        "\t\t- Use PCA or regularization techniques.\n",
        "\n",
        "- 13. What are some common techniques for transforming categorical variables for use in regression models\n",
        "\t\t- One-hot encoding\n",
        "\t\t- Label encoding\n",
        "\t\t- Dummy variable creation\n",
        "\n",
        "- 14. What is the role of interaction terms in Multiple Linear Regression\n",
        "\t\t- Interaction terms show combined effects of variables\n",
        "\t\t- on the dependent variable.\n",
        "\n",
        "- 15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
        "\t\t- In simple regression, it is Y when X equals zero.\n",
        "\t\t- In multiple regression, it is Y when all X values are zero.\n",
        "\n",
        "- 16. What is the significance of the slope in regression analysis, and how does it affect predictions\n",
        "\t\t- The slope shows the rate of change.\n",
        "\t\t- It directly affects predicted values.\n",
        "\n",
        "- 17. How does the intercept in a regression model provide context for the relationship between variables\n",
        "\t\t- The intercept gives a baseline value of the dependent variable.\n",
        "\n",
        "- 18. What are the limitations of using R² as a sole measure of model performance\n",
        "\t\t- It does not detect overfitting.\n",
        "\t\t- It does not show causation.\n",
        "\t\t- It always increases with more variables.\n",
        "\n",
        "- 19. How would you interpret a large standard error for a regression coefficient\n",
        "\t\t- A large standard error shows low confidence\n",
        "\t\t- in the estimated coefficient.\n",
        "\n",
        "- 20. How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
        "\t\t- It appears as a funnel-shaped pattern in residual plots.\n",
        "\t\t- It affects model accuracy and predictions.\n",
        "\n",
        "- 21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²\n",
        "\t\t- It indicates unnecessary variables are included in the model.\n",
        "\n",
        "- 22. Why is it important to scale variables in Multiple Linear Regression\n",
        "\t\t- Scaling ensures equal contribution of variables.\n",
        "\t\t- It improves model stability.\n",
        "\n",
        "- 23. What is polynomial regression\n",
        "\t\t- Polynomial regression models non-linear relationships\n",
        "\t\t- using polynomial terms.\n",
        "\n",
        "- 24. How does polynomial regression differ from linear regression\n",
        "\t\t- Linear regression fits a straight line.\n",
        "\t\t- Polynomial regression fits a curved line.\n",
        "\n",
        "- 25. When is polynomial regression used\n",
        "\t\t- It is used when data shows a curved relationship.\n",
        "\n",
        "- 26. What is the general equation for polynomial regression\n",
        "\t\t- Y = a0 + a1X + a2X² + ... + anXⁿ\n",
        "\n",
        "- 27. Can polynomial regression be applied to multiple variables\n",
        "\t\t- Yes, polynomial regression can be applied to multiple variables.\n",
        "\n",
        "- 28. What are the limitations of polynomial regression\n",
        "\t\t- Overfitting\n",
        "\t\t- Complex models\n",
        "\t\t- Poor performance outside data range\n",
        "\n",
        "- 29. What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",
        "\t\t- Cross-validation\n",
        "\t\t- Adjusted R²\n",
        "\t\t- Mean squared error\n",
        "\n",
        "- 30. Why is visualization important in polynomial regression\n",
        "\t\t- Visualization helps understand the curve.\n",
        "\t\t- It helps detect overfitting.\n",
        "\n",
        "- 31. How is polynomial regression implemented in Python\n",
        "\t\t- By creating polynomial features.\n",
        "\t\t- Then applying linear regression using scikit-learn.\n"
      ],
      "metadata": {
        "id": "6EEGu-Gw62dw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CflrU7iy63o0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}